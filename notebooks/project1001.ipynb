{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e63d76c",
   "metadata": {},
   "source": [
    "## Project 1001\n",
    "\n",
    "Does ones education level influence how quickly they will notice an intentional spike in vocabulary during a normal (text-based) conversation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec81cd",
   "metadata": {},
   "source": [
    "Notes 1: \n",
    "\n",
    "I have a 1001 Words for Success Quick Study tri-fold. \n",
    "This list can not \"technically\" be found online, so I scanned it using WhiteLines, I then uploaded it to Google Drive and opened as a Google Doc to automatically trigger the OCR. \n",
    "\n",
    "Unfortunately, the OCR was not perfect, and the scan also includes definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69028ab",
   "metadata": {},
   "source": [
    "Notes 2:\n",
    "\n",
    "I then began attempting to use AI to parse the OCR list and the PDF scan to create a spreadsheet with all of the words found. I used ChatGPT, Claude, and Gemini for this step. Each LLM gave me a different original input, which ChatGPT's being the lowest by far. \n",
    "###### ChatGPT: 587, Gemini: 989, Claude: 1009\n",
    "\n",
    "Which was interesting because...I expected 1001 words. This meant that Claude was pulling things that weren't intended to be one of the 1001 words, and the other two had a process that completely missed words. \n",
    "\n",
    "I made a table where each column was a different list of potential words for my final csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe963049",
   "metadata": {},
   "source": [
    "Notes 3:\n",
    "\n",
    "\n",
    "I needed to create a script that would remove words that can't be found in isolation in a dictionary, meaning \"apple\" would be fine, but \"apples are good\" would be rejected. I also needed to remove duplicates and create a new csv that showed me just the unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee65e74",
   "metadata": {},
   "source": [
    "Notes 4: \n",
    "\n",
    "This process took 4 total notebooks as I tried different iterations and then attempted to use Claude and ChatGPT to optimize new libraries, such as *enchant* which I learned of through this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d88c98",
   "metadata": {},
   "source": [
    "Notes 5:\n",
    "\n",
    "subjects.xlsx -> subjects to be tested\n",
    "\n",
    "ocr_dicts.xlsx -> original spreadsheet for notebook\n",
    "\n",
    "unique_words.csv -> output from final_dict_cleaner.ipynb, where ocr_dicts.xlsx is the input.\n",
    "\n",
    "invalid_words.xlsx -> words rejected by final_dict_cleaner\n",
    "\n",
    "unique_sort.py -> script used to sort csv + lowercase all words\n",
    "\n",
    "unique_words_sorted.csv -> output from unique_sort.py\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
