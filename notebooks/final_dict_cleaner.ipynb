{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8784667a",
   "metadata": {},
   "source": [
    "This is the 4th version of this notebook. All prior versions are saved for debugging and the inherent historical value. However, this is the only notebook that should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "395e5793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Cleaning DataFrame\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import enchant\n",
    "from tqdm import tqdm\n",
    "\n",
    "def print_step(step_number: int, message: str) -> None:\n",
    "    tqdm.write(f\"[Step {step_number}] {message}\")\n",
    "\n",
    "\n",
    "print_step(1, \"Cleaning DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdfa9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2] Config Loaded\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = Path(r\"C:\\\\Users\\\\joshu\\\\OneDrive - Georgia State University\\\\Dojo\\\\Projects\\\\Work\\\\Ruminations\\\\project1001\\\\data\\\\ocr_dicts.xlsx\")\n",
    "NUM_WORD_COLS = 3         \n",
    "DICT_LOCALE = \"en_US\"    \n",
    "\n",
    "print_step(2, \"Config Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f660147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3] Loading spreadsheet from C:\\Users\\joshu\\OneDrive - Georgia State University\\Dojo\\Projects\\Work\\Ruminations\\project1001\\data\\ocr_dicts.xlsx...\n",
      "[Step 4] Analyzing columns: Gemini, ChatGPT, Claude + Total rows: 1011 \n"
     ]
    }
   ],
   "source": [
    "print_step(3, f\"Loading spreadsheet from {INPUT_PATH}...\")\n",
    "\n",
    "if INPUT_PATH.suffix.lower() in {\".xlsx\", \".xls\"}:\n",
    "    df = pd.read_excel(INPUT_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Normalize whitespace on object columns\n",
    "for c in df.columns:\n",
    "    if pd.api.types.is_object_dtype(df[c]):\n",
    "        df[c] = df[c].astype(str).str.strip().replace({\"nan\": \"\"})\n",
    "\n",
    "# Pick the first N columns as word columns\n",
    "word_cols = list(df.columns[:NUM_WORD_COLS])\n",
    "\n",
    "# Friendly info (explicit col_a/b/c like you wanted)\n",
    "col_a, col_b, col_c = word_cols[0], word_cols[1], word_cols[2]\n",
    "print_step(4, f\"Analyzing columns: {col_a}, {col_b}, {col_c} + Total rows: {len(df)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "040e35f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5] Dictionary initialized for locale: en_US\n"
     ]
    }
   ],
   "source": [
    "d = enchant.Dict(DICT_LOCALE)\n",
    "\n",
    "def is_dict_entry(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if the entire (trimmed) cell is recognized by the dictionary.\n",
    "    Multi-word entries (e.g., 'Avant garde') are allowed if Enchant knows them.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    w = text.strip()\n",
    "    if not w:\n",
    "        return False\n",
    "    # be forgiving about casing\n",
    "    return d.check(w) or d.check(w.lower()) or d.check(w.capitalize())\n",
    "\n",
    "print_step(5, f\"Dictionary initialized for locale: {DICT_LOCALE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c7ca3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6] Validating & deduplicating words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns: 100%|██████████| 3/3 [00:00<00:00,  3.29col/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7] Kept 982 unique dictionary entries. Flagged 94 invalid entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_step(6, \"Validating & deduplicating words...\")\n",
    "\n",
    "seen_lower = set()      # case-insensitive dedupe\n",
    "unique_words = []       # preserve first-seen surface form\n",
    "invalid_rows = []       # for an audit sheet (RowIndex, Column, Value)\n",
    "\n",
    "for col in tqdm(word_cols, desc=\"Columns\", unit=\"col\"):\n",
    "    series = df[col].fillna(\"\").astype(str)\n",
    "    for i, raw in tqdm(series.items(), total=len(series), desc=f\"{col}\", unit=\"cells\", leave=False):\n",
    "        w = raw.strip()\n",
    "        if not w:\n",
    "            continue\n",
    "        if is_dict_entry(w):\n",
    "            key = w.lower()\n",
    "            if key not in seen_lower:\n",
    "                seen_lower.add(key)\n",
    "                unique_words.append(w)\n",
    "        else:\n",
    "            invalid_rows.append({\"RowIndex\": i, \"Column\": col, \"Value\": w})\n",
    "\n",
    "print_step(7, f\"Kept {len(unique_words)} unique dictionary entries. Flagged {len(invalid_rows)} invalid entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4caf805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8] Saving outputs next to the input file...\n",
      "[Step 8] Saved:\n",
      "       • C:\\Users\\joshu\\OneDrive - Georgia State University\\Dojo\\Projects\\Work\\Ruminations\\project1001\\data\\unique_words.csv\n",
      "       • C:\\Users\\joshu\\OneDrive - Georgia State University\\Dojo\\Projects\\Work\\Ruminations\\project1001\\data\\unique_dictionary_words.csv\n",
      "       • C:\\Users\\joshu\\OneDrive - Georgia State University\\Dojo\\Projects\\Work\\Ruminations\\project1001\\data\\invalid_words.xlsx\n",
      "[Step 9] Processing complete.\n"
     ]
    }
   ],
   "source": [
    "print_step(8, \"Saving outputs next to the input file...\")\n",
    "\n",
    "out_dir = INPUT_PATH.parent\n",
    "\n",
    "# 1) One word per row (your requested primary artifact)\n",
    "unique_words_df = pd.DataFrame({\"Word\": unique_words})\n",
    "one_per_row_csv = out_dir / \"unique_words.csv\"\n",
    "unique_words_df.to_csv(one_per_row_csv, index=False)\n",
    "\n",
    "# 2) Single row CSV (optional bonus format)\n",
    "one_row_csv = out_dir / \"unique_dictionary_words.csv\"\n",
    "pd.DataFrame([unique_words]).to_csv(one_row_csv, index=False, header=False)\n",
    "\n",
    "# 3) Invalids audit sheet\n",
    "invalid_xlsx = out_dir / \"invalid_words.xlsx\"\n",
    "if len(invalid_rows) > 0:\n",
    "    pd.DataFrame(invalid_rows).to_excel(invalid_xlsx, index=False)\n",
    "else:\n",
    "    # create an empty file with headers so it's obvious the script ran\n",
    "    pd.DataFrame(columns=[\"RowIndex\", \"Column\", \"Value\"]).to_excel(invalid_xlsx, index=False)\n",
    "\n",
    "print_step(8, \"Saved:\")\n",
    "tqdm.write(f\"       • {one_per_row_csv}\")\n",
    "tqdm.write(f\"       • {one_row_csv}\")\n",
    "tqdm.write(f\"       • {invalid_xlsx}\")\n",
    "print_step(9, \"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
